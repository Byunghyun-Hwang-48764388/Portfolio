{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sample for each class of train dataset: {'fake': 153, 'real': 153}\n",
      "Number of sample for each class of test dataset: {'fake': 110, 'real': 110}\n",
      "Balanced Train size: 245\n",
      "Balanced Val size: 61\n",
      "Balanced Test size: 220\n",
      "Epoch 1/20, Loss: 0.6318, Val Acc: 78.69%\n",
      "Epoch 2/20, Loss: 0.3797, Val Acc: 80.33%\n",
      "Epoch 3/20, Loss: 0.2615, Val Acc: 83.61%\n",
      "Epoch 4/20, Loss: 0.1706, Val Acc: 81.97%\n",
      "Epoch 5/20, Loss: 0.1499, Val Acc: 83.61%\n",
      "Epoch 6/20, Loss: 0.1267, Val Acc: 86.89%\n",
      "Epoch 7/20, Loss: 0.0938, Val Acc: 85.25%\n",
      "Epoch 8/20, Loss: 0.0974, Val Acc: 77.05%\n",
      "Epoch 9/20, Loss: 0.1292, Val Acc: 83.61%\n",
      "Epoch 10/20, Loss: 0.1036, Val Acc: 78.69%\n",
      "Epoch 11/20, Loss: 0.0768, Val Acc: 83.61%\n",
      "Epoch 12/20, Loss: 0.0785, Val Acc: 83.61%\n",
      "Epoch 13/20, Loss: 0.0913, Val Acc: 75.41%\n",
      "Epoch 14/20, Loss: 0.1129, Val Acc: 80.33%\n",
      "Epoch 15/20, Loss: 0.1092, Val Acc: 86.89%\n",
      "Epoch 16/20, Loss: 0.0976, Val Acc: 88.52%\n",
      "Epoch 17/20, Loss: 0.0855, Val Acc: 86.89%\n",
      "Epoch 18/20, Loss: 0.0641, Val Acc: 83.61%\n",
      "Epoch 19/20, Loss: 0.0909, Val Acc: 86.89%\n",
      "Epoch 20/20, Loss: 0.0897, Val Acc: 80.33%\n",
      "Test Accuracy: 80.9090909090909\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# ----------------------------\n",
    "# Setup data roots and randomness\n",
    "# ----------------------------\n",
    "train_dir = \"root1\"\n",
    "test_dir  = \"root2\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 32\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Apply downsampling to balance data\n",
    "# ----------------------------\n",
    "def balance_dataset(folder):\n",
    "    class_folders = [f for f in os.listdir(folder) if os.path.isdir(os.path.join(folder, f))]\n",
    "    \n",
    "    class_files = {}\n",
    "    for cls in class_folders:\n",
    "        path = os.path.join(folder, cls)\n",
    "        files = [os.path.join(path, f) for f in os.listdir(path)\n",
    "                 if f.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".bmp\", \".webp\"))]\n",
    "        class_files[cls] = files\n",
    "\n",
    "    min_count = min(len(files) for files in class_files.values())\n",
    "\n",
    "    balanced_files = []\n",
    "    balanced_labels = []\n",
    "    for idx, cls in enumerate(class_folders):\n",
    "        sampled = random.sample(class_files[cls], min_count)\n",
    "        balanced_files.extend(sampled)\n",
    "        balanced_labels.extend([idx] * len(sampled))\n",
    "    \n",
    "    return balanced_files, balanced_labels, class_folders\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Balance train / test data\n",
    "# ----------------------------\n",
    "train_files, train_labels, class_names = balance_dataset(train_dir)\n",
    "test_files, test_labels, _ = balance_dataset(test_dir)\n",
    "\n",
    "print(\"Number of sample for each class of train dataset:\", {class_names[i]: train_labels.count(i) for i in range(len(class_names))})\n",
    "print(\"Number of sample for each class of test dataset:\", {class_names[i]: test_labels.count(i) for i in range(len(class_names))})\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset class\n",
    "# ----------------------------\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_paths[idx])\n",
    "\n",
    "        if img.mode in (\"P\", \"RGBA\"):\n",
    "            img = img.convert(\"RGB\")\n",
    "        else:\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Transform\n",
    "# ----------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Dataset & Dataloader\n",
    "# ----------------------------\n",
    "train_dataset = CustomDataset(train_files, train_labels, transform=train_transform)\n",
    "test_dataset  = CustomDataset(test_files, test_labels, transform=test_transform)\n",
    "\n",
    "# ---- train / val (80:20) ---- #\n",
    "total_train = len(train_dataset)\n",
    "val_size = int(total_train * 0.2)\n",
    "train_size = total_train - val_size\n",
    "\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_subset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Balanced Train size:\", len(train_subset))\n",
    "print(\"Balanced Val size:\", len(val_subset))\n",
    "print(\"Balanced Test size:\", len(test_dataset))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Model\n",
    "# ----------------------------\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Optimizer & Scheduler\n",
    "# ----------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Training / Validation function\n",
    "# ----------------------------\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total * 100\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Training Loop\n",
    "# ----------------------------\n",
    "for epoch in range(20):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/20, Loss: {loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Test Accuracy\n",
    "# ----------------------------\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
